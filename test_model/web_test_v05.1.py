from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException, StaleElementReferenceException
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from itertools import product
import string
import time
import csv
import logging
import os
import random
import re
import sys
import json
from bs4 import BeautifulSoup

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)
DEBUG_MODE = False

def set_debug_mode(enabled=True):
    """‡πÄ‡∏õ‡∏¥‡∏î/‡∏õ‡∏¥‡∏î debug mode"""
    global DEBUG_MODE
    DEBUG_MODE = enabled
    logger.setLevel(logging.DEBUG if enabled else logging.INFO)

def debug_print(message):
    """‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° debug"""
    if DEBUG_MODE:
        print(f"üîç DEBUG: {message}")

def generate_keywords():
    """‡∏≠‡πà‡∏≤‡∏ô‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡∏¢‡∏≤‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ ‡πÇ‡∏î‡∏¢‡∏î‡∏∂‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ä‡∏∑‡πà‡∏≠‡∏¢‡∏≤‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô double quotes"""
    try:
        import re
        with open('Name_Ya_all.txt', 'r', encoding='utf-8') as f:
            content = f.read()
            
            # ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
            total_lines = len(content.splitlines())
            logger.info(f"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå: {total_lines}")
            
            # ‡∏î‡∏∂‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô double quotes
            drug_names = re.findall(r'"([^"]*)"', content)
            logger.info(f"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏¢‡∏≤‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢ quotes: {len(drug_names)}")
            
            # ‡πÄ‡∏Å‡πá‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡∏¢‡∏≤‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡∏±‡∏î‡∏ó‡∏¥‡πâ‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö
            skipped_names = []
            
            # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏ä‡∏∑‡πà‡∏≠‡∏¢‡∏≤ - ‡πÄ‡∏≠‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏•‡∏±‡∏Å‡∏Å‡πà‡∏≠‡∏ô‡∏ß‡∏á‡πÄ‡∏•‡πá‡∏ö
            cleaned_names = []
            for name in drug_names:
                # ‡πÅ‡∏¢‡∏Å‡πÄ‡∏≠‡∏≤‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏•‡∏±‡∏Å‡∏Å‡πà‡∏≠‡∏ô‡∏ß‡∏á‡πÄ‡∏•‡πá‡∏ö
                main_name = name.split('(')[0].strip()
                # ‡∏ñ‡πâ‡∏≤‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤ ‡πÉ‡∏´‡πâ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏Ç‡πâ‡∏≤ list
                if main_name:
                    cleaned_names.append(main_name)
                else:
                    skipped_names.append(name)
            
            logger.info(f"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏¢‡∏≤‡∏´‡∏•‡∏±‡∏á‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î: {len(cleaned_names)}")
            
            if skipped_names:
                logger.warning(f"‡∏°‡∏µ‡∏ä‡∏∑‡πà‡∏≠‡∏¢‡∏≤‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏Ç‡πâ‡∏≤‡∏° {len(skipped_names)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
                # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ä‡∏∑‡πà‡∏≠‡∏¢‡∏≤‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏Ç‡πâ‡∏≤‡∏°‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå
                with open('skipped_drug_names.txt', 'w', encoding='utf-8') as f:
                    for name in skipped_names:
                        f.write(f"{name}\n")
                logger.info("‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡∏¢‡∏≤‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏Ç‡πâ‡∏≤‡∏°‡πÑ‡∏ß‡πâ‡πÉ‡∏ô skipped_drug_names.txt")
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ä‡∏∑‡πà‡∏≠‡∏¢‡∏≤‡∏ó‡∏µ‡πà‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô ‡πÅ‡∏ï‡πà‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
            name_counts = {}
            for name in cleaned_names:
                name_counts[name] = name_counts.get(name, 0) + 1
            
            duplicates = {name: count for name, count in name_counts.items() if count > 1}
            if duplicates:
                logger.info(f"‡∏û‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡∏¢‡∏≤‡∏ó‡∏µ‡πà‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô {len(duplicates)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ (‡πÅ‡∏ï‡πà‡∏à‡∏∞‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î)")
                # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ä‡∏∑‡πà‡∏≠‡∏¢‡∏≤‡∏ó‡∏µ‡πà‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå
                with open('duplicate_drug_names.txt', 'w', encoding='utf-8') as f:
                    for name, count in sorted(duplicates.items()):
                        f.write(f"{name} (‡∏ã‡πâ‡∏≥ {count} ‡∏Ñ‡∏£‡∏±‡πâ‡∏á)\n")
                logger.info("‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡∏¢‡∏≤‡∏ó‡∏µ‡πà‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô‡πÑ‡∏ß‡πâ‡πÉ‡∏ô duplicate_drug_names.txt")
            
            return cleaned_names  # ‡∏™‡πà‡∏á‡∏Ñ‡∏∑‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏¢‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î ‡∏£‡∏ß‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏ó‡∏µ‡πà‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô
            
    except Exception as e:
        logger.error(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡∏¢‡∏≤: {e}")
        # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏ß‡∏¥‡∏ò‡∏µ‡πÄ‡∏î‡∏¥‡∏°‡πÄ‡∏õ‡πá‡∏ô fallback
        return [''.join(p) for p in product(string.ascii_lowercase, repeat=2)]

def print_drug_data(data, index=None):
    """‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏¢‡∏≤‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡∏≠‡πà‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢"""
    print("\n" + "="*80)
    print(f"üíä ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏¢‡∏≤‡∏ó‡∏µ‡πà {index}" if index else "üíä ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏¢‡∏≤")
    print("="*80)
    
    # ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
    basic_info = ["‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏°‡∏±‡∏ç", "‡∏ä‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏≤", "‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏¢‡∏≤"]
    for key in basic_info:
        value = data.get(key, "")
        status = "üîπ" if value else "‚ùå"
        print(f"{status} {key}: {value if value else '(‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•)'}")
    
    print("-" * 80)
    
    # ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î
    detail_keys = [
        "‡∏¢‡∏≤‡∏ô‡∏µ‡πâ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö", "‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏¢‡∏≤", "‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏£‡πÅ‡∏à‡πâ‡∏á‡πÉ‡∏´‡πâ‡πÅ‡∏û‡∏ó‡∏¢‡πå‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏†‡∏™‡∏±‡∏ä‡∏Å‡∏£‡∏ó‡∏£‡∏≤‡∏ö",
        "‡∏ó‡∏≥‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£‡∏´‡∏≤‡∏Å‡∏•‡∏∑‡∏°‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏ó‡∏≤‡∏ô‡∏¢‡∏≤‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏ä‡πâ‡∏¢‡∏≤", "‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡πÑ‡∏°‡πà‡∏û‡∏∂‡∏á‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ",
        "‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡πÑ‡∏°‡πà‡∏û‡∏∂‡∏á‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏à‡πâ‡∏á‡πÅ‡∏û‡∏ó‡∏¢‡πå‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏†‡∏™‡∏±‡∏ä‡∏Å‡∏£‡∏ó‡∏±‡∏ô‡∏ó‡∏µ", "‡∏Å‡∏≤‡∏£‡πÄ‡∏Å‡πá‡∏ö‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏¢‡∏≤"
    ]
    
    for key in detail_keys:
        value = data.get(key, "")
        print(f"\nüî∏ {key}:")
        if value:
            for line in value.split('\n'):
                if line.strip():
                    print(f"   {line.strip()}")
        else:
            print("   (‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•)")
    
    if data.get("URL"):
        print(f"\nüîó URL: {data['URL']}")
    print("=" * 80)

def extract_full_text_with_linebreaks(driver):
    """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å HTML ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà <br> ‡∏î‡πâ‡∏ß‡∏¢ \n"""
    soup = BeautifulSoup(driver.page_source, "html.parser")
    for br in soup.find_all("br"):
        br.replace_with("\n")
    return soup.get_text(separator="\n")

def print_summary_stats(all_drugs, keyword, current_index):
    """‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏™‡∏£‡∏∏‡∏õ"""
    complete_data = sum(1 for drug in all_drugs if any(drug.get(field, "").strip() 
                       for field in ["‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏°‡∏±‡∏ç", "‡∏ä‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏≤", "‡∏¢‡∏≤‡∏ô‡∏µ‡πâ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö"]))
    
    print(f"\nüìä ‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Keyword: {keyword})")
    print(f"üìà ‡∏£‡∏ß‡∏°‡∏¢‡∏≤‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á‡∏°‡∏≤‡πÑ‡∏î‡πâ: {len(all_drugs)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
    print(f"‚úÖ ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô: {complete_data} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
    print(f"‚ö†Ô∏è ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡∏Ñ‡∏£‡∏ö: {len(all_drugs) - complete_data} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
    print("-" * 50)

def extract_drug_usage_patterns(text):
    """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏¢‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢"""
    patterns = {
        'general': [
            r'(?:‡∏¢‡∏≤‡∏ô‡∏µ‡πâ‡πÉ‡∏ä‡πâ|‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö|‡πÉ‡∏ä‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠|‡πÉ‡∏ä‡πâ‡∏£‡∏±‡∏Å‡∏©‡∏≤)[^.]*?(?=\.|$)',
            r'(?:‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏ö‡πà‡∏á‡πÉ‡∏ä‡πâ|‡∏°‡∏µ‡∏™‡∏£‡∏£‡∏û‡∏Ñ‡∏∏‡∏ì)[^.]*?(?=\.|$)',
            r'(?:‡πÉ‡∏ä‡πâ‡∏ö‡∏£‡∏£‡πÄ‡∏ó‡∏≤|‡πÉ‡∏ä‡πâ‡∏ö‡∏≥‡∏ö‡∏±‡∏î)[^.]*?(?=\.|$)'
        ],
        'specific_conditions': [
            r'(?:‡∏£‡∏±‡∏Å‡∏©‡∏≤‡πÇ‡∏£‡∏Ñ|‡∏ö‡∏£‡∏£‡πÄ‡∏ó‡∏≤‡∏≠‡∏≤‡∏Å‡∏≤‡∏£)[^.]*?(?=\.|$)',
            r'(?:‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏≠‡∏≤‡∏Å‡∏≤‡∏£|‡∏•‡∏î‡∏≠‡∏≤‡∏Å‡∏≤‡∏£)[^.]*?(?=\.|$)',
            r'(?:‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô|‡∏£‡∏±‡∏Å‡∏©‡∏≤)[^.]*?(?=\.|$)'
        ],
        'combinations': [
            r'(?:‡πÉ‡∏ä‡πâ‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ö|‡πÉ‡∏ä‡πâ‡πÄ‡∏™‡∏£‡∏¥‡∏°‡∏Å‡∏±‡∏ö)[^.]*?(?=\.|$)',
            r'(?:‡∏≠‡∏≤‡∏à‡πÉ‡∏ä‡πâ‡∏£‡πà‡∏ß‡∏°|‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ‡∏£‡πà‡∏ß‡∏°)[^.]*?(?=\.|$)'
        ],
        'special_cases': [
            r'(?:‡∏Å‡∏£‡∏ì‡∏µ‡∏û‡∏¥‡πÄ‡∏®‡∏©|‡πÉ‡∏ô‡∏Å‡∏£‡∏ì‡∏µ)[^.]*?(?=\.|$)',
            r'(?:‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏£‡∏£‡∏∞‡∏ß‡∏±‡∏á|‡∏Ç‡πâ‡∏≠‡∏´‡πâ‡∏≤‡∏°‡πÉ‡∏ä‡πâ)[^.]*?(?=\.|$)'
        ]
    }
    
    results = []
    seen = set()
    
    for category, pattern_list in patterns.items():
        for pattern in pattern_list:
            matches = re.finditer(pattern, text, re.IGNORECASE | re.MULTILINE)
            for match in matches:
                content = match.group().strip()
                if len(content) > 20 and content not in seen:
                    results.append(content)
                    seen.add(content)
    
    return results

def extract_administration_patterns(text):
    """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏¢‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢"""
    patterns = {
        'oral': [
            r'(?:‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏ó‡∏≤‡∏ô|‡∏Å‡∏¥‡∏ô)[^.]*?(?:‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏•‡∏∞|‡∏ß‡∏±‡∏ô‡∏•‡∏∞|‡∏ó‡∏∏‡∏Å|‡∏°‡∏∑‡πâ‡∏≠)[^.]*?(?=\.|$)',
            r'(?:‡∏Ñ‡∏ß‡∏£‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏ó‡∏≤‡∏ô|‡πÉ‡∏´‡πâ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏ó‡∏≤‡∏ô)[^.]*?(?=\.|$)'
        ],
        'injection': [
            r'(?:‡∏â‡∏µ‡∏î|‡πÉ‡∏´‡πâ‡∏¢‡∏≤‡∏ó‡∏≤‡∏á)[^.]*?(?:‡πÄ‡∏Ç‡πâ‡∏≤|‡∏ó‡∏≤‡∏á|‡πÉ‡∏ï‡πâ)[^.]*?(?=\.|$)',
            r'(?:‡∏Ç‡∏ô‡∏≤‡∏î‡∏¢‡∏≤‡∏â‡∏µ‡∏î|‡∏ß‡∏¥‡∏ò‡∏µ‡∏â‡∏µ‡∏î)[^.]*?(?=\.|$)'
        ],
        'topical': [
            r'(?:‡∏ó‡∏≤|‡∏õ‡πâ‡∏≤‡∏¢|‡∏û‡πà‡∏ô)[^.]*?(?:‡∏ö‡∏£‡∏¥‡πÄ‡∏ß‡∏ì|‡∏ú‡∏¥‡∏ß‡∏´‡∏ô‡∏±‡∏á|‡πÅ‡∏ú‡∏•)[^.]*?(?=\.|$)',
            r'(?:‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏≤|‡∏Å‡∏≤‡∏£‡∏ó‡∏≤)[^.]*?(?=\.|$)'
        ],
        'timing': [
            r'(?:‡∏ó‡∏∏‡∏Å|‡∏Å‡πà‡∏≠‡∏ô|‡∏´‡∏•‡∏±‡∏á)[^.]*?(?:‡∏°‡∏∑‡πâ‡∏≠‡∏≠‡∏≤‡∏´‡∏≤‡∏£|‡∏ô‡∏≤‡∏ó‡∏µ|‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á)[^.]*?(?=\.|$)',
            r'(?:‡πÄ‡∏ß‡∏•‡∏≤|‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤)[^.]*?(?=\.|$)'
        ],
        'dosage': [
            r'(?:‡∏Ç‡∏ô‡∏≤‡∏î‡∏¢‡∏≤|‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì)[^.]*?(?:‡∏ï‡∏≤‡∏°|‡∏Ç‡∏∂‡πâ‡∏ô‡∏Å‡∏±‡∏ö)[^.]*?(?=\.|$)',
            r'(?:‡∏ú‡∏π‡πâ‡πÉ‡∏´‡∏ç‡πà|‡πÄ‡∏î‡πá‡∏Å)[^.]*?(?:‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏ó‡∏≤‡∏ô|‡πÉ‡∏ä‡πâ)[^.]*?(?=\.|$)'
        ],
        'special_instructions': [
            r'(?:‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏£‡∏£‡∏∞‡∏ß‡∏±‡∏á|‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥)[^.]*?(?=\.|$)',
            r'(?:‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏|‡∏Ç‡πâ‡∏≠‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï)[^.]*?(?=\.|$)'
        ]
    }
    
    results = []
    seen = set()
    
    for category, pattern_list in patterns.items():
        for pattern in pattern_list:
            matches = re.finditer(pattern, text, re.IGNORECASE | re.MULTILINE)
            for match in matches:
                content = match.group().strip()
                if len(content) > 20 and content not in seen:
                    results.append(content)
                    seen.add(content)
    
    return results

def clean_and_format_content(content_list):
    """‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤"""
    cleaned = []
    seen = set()
    
    for item in content_list:
        # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
        clean_item = re.sub(r'\s+', ' ', item).strip()
        clean_item = re.sub(r'^[‚Ä¢\-\*\s]+', '', clean_item)
        
        # ‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
        unwanted = ["‡πÑ‡∏î‡πâ‡πÅ‡∏Å‡πà", "‡πÄ‡∏ä‡πà‡∏ô", "‡∏≠‡∏≤‡∏ó‡∏¥", "‡∏Ñ‡∏∑‡∏≠", "‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ"]
        for word in unwanted:
            clean_item = clean_item.replace(word, "")
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ã‡πâ‡∏≥‡∏ã‡πâ‡∏≠‡∏ô
        if len(clean_item) > 20 and clean_item not in seen:
            cleaned.append(clean_item)
            seen.add(clean_item)
    
    return cleaned

def extract_drug_usage_section(lines, start_idx, section_indices):
    """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• '‡∏¢‡∏≤‡∏ô‡∏µ‡πâ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö' ‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°"""
    try:
        # ‡∏´‡∏≤‡∏Ç‡∏≠‡∏ö‡πÄ‡∏Ç‡∏ï‡∏Ç‡∏≠‡∏á‡∏™‡πà‡∏ß‡∏ô
        end_idx = find_section_end(lines, start_idx, section_indices)
        
        # ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤
        content = gather_section_content(lines, start_idx, end_idx)
        
        if not content:
            return ""
        
        # ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤
        combined_text = " ".join(content)
        
        # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡πâ‡∏ß‡∏¢‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ï‡πà‡∏≤‡∏á‡πÜ
        usage_patterns = extract_drug_usage_patterns(combined_text)
        
        # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö
        cleaned_patterns = clean_and_format_content(usage_patterns)
        
        if cleaned_patterns:
            return "\n".join([f"* {pattern}" for pattern in cleaned_patterns])
        
        return ""
        
    except Exception as e:
        debug_print(f"Usage extraction error: {e}")
        return ""

def extract_administration_section(lines, start_idx, section_indices):
    """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• '‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏¢‡∏≤' ‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°"""
    try:
        # ‡∏´‡∏≤‡∏Ç‡∏≠‡∏ö‡πÄ‡∏Ç‡∏ï‡∏Ç‡∏≠‡∏á‡∏™‡πà‡∏ß‡∏ô
        end_idx = find_section_end(lines, start_idx, section_indices)
        
        # ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤
        content = gather_section_content(lines, start_idx, end_idx)
        
        if not content:
            return ""
        
        # ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤
        combined_text = " ".join(content)
        debug_print(f"Administration content length: {len(combined_text)}")
        
        # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡πâ‡∏ß‡∏¢‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ï‡πà‡∏≤‡∏á‡πÜ
        admin_patterns = extract_administration_patterns(combined_text)
        
        # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö
        cleaned_patterns = clean_and_format_content(admin_patterns)
        
        if cleaned_patterns:
            return "\n".join([f"* {pattern}" for pattern in cleaned_patterns])
        
        # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÉ‡∏´‡∏°‡πà ‡πÉ‡∏ä‡πâ‡∏ß‡∏¥‡∏ò‡∏µ‡πÄ‡∏î‡∏¥‡∏°
        bullets = extract_administration_bullets_comprehensive(combined_text)
        if bullets:
            return "\n".join([f"* {bullet}" for bullet in bullets])
        
        return ""
        
    except Exception as e:
        debug_print(f"Administration extraction error: {e}")
        return ""

def find_section_end(lines, start_idx, section_indices):
    """‡∏´‡∏≤‡∏à‡∏∏‡∏î‡∏™‡∏¥‡πâ‡∏ô‡∏™‡∏∏‡∏î‡∏Ç‡∏≠‡∏á‡∏™‡πà‡∏ß‡∏ô"""
    end_idx = len(lines)
    next_section_keywords = [
        "‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏¢‡∏≤", "‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏£‡πÅ‡∏à‡πâ‡∏á", "‡∏ó‡∏≥‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£",
        "‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡πÑ‡∏°‡πà‡∏û‡∏∂‡∏á‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå", "‡∏Å‡∏≤‡∏£‡πÄ‡∏Å‡πá‡∏ö‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏¢‡∏≤"
    ]
    
    # ‡∏´‡∏≤‡∏à‡∏∏‡∏î‡∏™‡∏¥‡πâ‡∏ô‡∏™‡∏∏‡∏î‡∏à‡∏≤‡∏Å section_indices
    for section_name, idx in section_indices.items():
        if idx > start_idx:
            end_idx = min(end_idx, idx)
    
    # ‡∏´‡∏≤‡∏à‡∏∏‡∏î‡∏™‡∏¥‡πâ‡∏ô‡∏™‡∏∏‡∏î‡∏à‡∏≤‡∏Å‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
    for i in range(start_idx + 1, end_idx):
        if i >= len(lines):
            break
        if any(keyword in lines[i] for keyword in next_section_keywords):
            end_idx = i
            break
    
    return end_idx

def gather_section_content(lines, start_idx, end_idx):
    """‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏Ç‡∏≠‡∏á‡∏™‡πà‡∏ß‡∏ô"""
    content = []
    current_idx = start_idx + 1
    
    while current_idx < min(end_idx, len(lines)):
        line = lines[current_idx].strip()
        
        # ‡∏Ç‡πâ‡∏≤‡∏°‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ß‡πà‡∏≤‡∏á
        if not line:
            current_idx += 1
            continue
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö unwanted line
        if not is_unwanted_line(line):
            # ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏±‡∏ö HTML list
            if line.startswith('<li>'):
                list_item = []
                while current_idx < len(lines) and '</li>' not in lines[current_idx]:
                    list_item.append(lines[current_idx])
                    current_idx += 1
                if current_idx < len(lines):
                    list_item.append(lines[current_idx])
                content.append('\n'.join(list_item))
            else:
                content.append(line)
        
        current_idx += 1
    
    return content

def extract_trade_name_comprehensive(driver, soup):
    """‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏≤‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°"""
    trade_name = ""
    
    try:
        # 1. ‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å h1 ‡∏´‡∏£‡∏∑‡∏≠ h2 ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ "‡∏ä‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏≤"
        for tag in ["h1", "h2"]:
            headers = soup.find_all(tag)
            for header in headers:
                if "‡∏ä‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏≤" in header.text:
                    # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏±‡∏á‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ "‡∏ä‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏≤"
                    text = header.text.split("‡∏ä‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏≤")[-1].strip()
                    if text and len(text) > 1:
                        return clean_single_line_content(text)
        
        # 2. ‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å div.bs-callout ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ "‡∏ä‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏≤"
        callouts = soup.find_all("div", class_="bs-callout")
        for callout in callouts:
            if "‡∏ä‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏≤" in callout.text:
                # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏±‡∏á‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ "‡∏ä‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏≤"
                text = callout.text.split("‡∏ä‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏≤")[-1].strip()
                if text and len(text) > 1:
                    return clean_single_line_content(text)
        
        # 3. ‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å strong tag ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ "‡∏ä‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏≤"
        strongs = soup.find_all("strong")
        for strong in strongs:
            if "‡∏ä‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏≤" in strong.text:
                # ‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
                next_sibling = strong.next_sibling
                if next_sibling:
                    text = next_sibling.strip()
                    if text and len(text) > 1:
                        return clean_single_line_content(text)
        
        # 4. ‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å table ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ "‡∏ä‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏≤"
        tables = soup.find_all("table")
        for table in tables:
            rows = table.find_all("tr")
            for row in rows:
                cells = row.find_all("td")
                if len(cells) >= 2 and "‡∏ä‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏≤" in cells[0].text:
                    text = cells[1].text.strip()
                    if text and len(text) > 1:
                        return clean_single_line_content(text)
        
        # 5. ‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡πÄ‡∏û‡∏à
        title = driver.title
        if title:
            # ‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ "‡∏¢‡∏≤" ‡∏≠‡∏≠‡∏Å‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
            title = title.replace("‡∏¢‡∏≤", "").strip()
            # ‡∏ï‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏±‡∏á | ‡∏´‡∏£‡∏∑‡∏≠ - ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
            for separator in ["|", "-", ":", "/"]:
                if separator in title:
                    title = title.split(separator)[0].strip()
            if title and len(title) > 1:
                return clean_single_line_content(title)
        
        # 6. ‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å meta tags
        meta_title = soup.find("meta", property="og:title")
        if meta_title and meta_title.get("content"):
            text = meta_title["content"].strip()
            # ‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ "‡∏¢‡∏≤" ‡∏≠‡∏≠‡∏Å‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
            text = text.replace("‡∏¢‡∏≤", "").strip()
            # ‡∏ï‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏±‡∏á | ‡∏´‡∏£‡∏∑‡∏≠ - ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
            for separator in ["|", "-", ":", "/"]:
                if separator in text:
                    text = text.split(separator)[0].strip()
            if text and len(text) > 1:
                return clean_single_line_content(text)
                
    except Exception as e:
        if DEBUG_MODE:
            debug_print(f"Trade name extraction error: {e}")
    
    return trade_name

def extract_drug_details_fixed(driver):
    """‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏¢‡∏≤‡∏´‡∏•‡∏±‡∏Å"""
    data = {
        "‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏°‡∏±‡∏ç": "", "‡∏ä‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏≤": "", "‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏¢‡∏≤": "", "‡∏¢‡∏≤‡∏ô‡∏µ‡πâ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö": "",
        "‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏¢‡∏≤": "", "‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏£‡πÅ‡∏à‡πâ‡∏á‡πÉ‡∏´‡πâ‡πÅ‡∏û‡∏ó‡∏¢‡πå‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏†‡∏™‡∏±‡∏ä‡∏Å‡∏£‡∏ó‡∏£‡∏≤‡∏ö": "",
        "‡∏ó‡∏≥‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£‡∏´‡∏≤‡∏Å‡∏•‡∏∑‡∏°‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏ó‡∏≤‡∏ô‡∏¢‡∏≤‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏ä‡πâ‡∏¢‡∏≤": "", "‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡πÑ‡∏°‡πà‡∏û‡∏∂‡∏á‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ": "",
        "‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡πÑ‡∏°‡πà‡∏û‡∏∂‡∏á‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏à‡πâ‡∏á‡πÅ‡∏û‡∏ó‡∏¢‡πå‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏†‡∏™‡∏±‡∏ä‡∏Å‡∏£‡∏ó‡∏±‡∏ô‡∏ó‡∏µ": "", "‡∏Å‡∏≤‡∏£‡πÄ‡∏Å‡πá‡∏ö‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏¢‡∏≤": "",
        "URL": driver.current_url
    }

    try:
        WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.TAG_NAME, "body")))
        time.sleep(2)

        # ‡πÉ‡∏ä‡πâ BeautifulSoup ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏¢‡∏Å‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå HTML
        html_content = driver.page_source
        
        # ‡∏•‡∏ö <b>‡∏°‡∏µ‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ</b> ‡πÅ‡∏•‡∏∞‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Å‡∏±‡∏ô‡∏≠‡∏≠‡∏Å
        html_content = re.sub(r'<b>\s*‡∏°‡∏µ‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ\s*</b>', '', html_content)
        html_content = re.sub(r'<strong>\s*‡∏°‡∏µ‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ\s*</strong>', '', html_content)
        html_content = re.sub(r'<b>\s*‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ\s*</b>', '', html_content)
        html_content = re.sub(r'<strong>\s*‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ\s*</strong>', '', html_content)
        
        soup = BeautifulSoup(html_content, "html.parser")

        # ‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏°‡∏±‡∏ç‡∏Å‡πà‡∏≠‡∏ô - ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡πÉ‡∏´‡∏°‡πà
        data["‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏°‡∏±‡∏ç"] = extract_generic_name_comprehensive(driver, soup)
        
        # ‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏≤
        data["‡∏ä‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏≤"] = extract_trade_name_comprehensive(driver, soup)
        
        # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å div.bs-callout
        callouts = soup.find_all("div", class_="bs-callout bs-callout-warning")
        
        for callout in callouts:
            # ‡∏î‡∏∂‡∏á‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏à‡∏≤‡∏Å h4
            header = callout.find("h4")
            if not header:
                continue
                
            # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠
            header_text = header.get_text(strip=True)
            header_text = re.sub(r'\s*<[^>]+>\s*', '', header_text)  # ‡∏•‡∏ö HTML tags
            header_text = re.sub(r'\s*\([^)]*\)\s*', '', header_text)  # ‡∏•‡∏ö‡∏ß‡∏á‡πÄ‡∏•‡πá‡∏ö‡πÅ‡∏•‡∏∞‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏ô‡∏ß‡∏á‡πÄ‡∏•‡πá‡∏ö
            
            # ‡∏Ç‡πâ‡∏≤‡∏°‡∏™‡πà‡∏ß‡∏ô footer ‡πÅ‡∏•‡∏∞ disclaimer
            if any(skip in header_text.lower() for skip in ["‡∏•‡∏¥‡∏Ç‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå", "copyright", "disclaimer"]):
                continue
                
            # ‡∏´‡∏≤‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
            matching_key = None
            for key in data.keys():
                if key != "URL" and key in header_text:
                    matching_key = key
                    break
                    
            if not matching_key:
                continue
                
            # ‡∏î‡∏∂‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤
            content = []
            
            # ‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å bullet points ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
            bullets = callout.find_all("li")
            if bullets:
                for bullet in bullets:
                    text = bullet.get_text(strip=True)
                    if text and not any(skip in text.lower() for skip in ["‡∏´‡∏ô‡πâ‡∏≤‡∏´‡∏•‡∏±‡∏Å", "‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠‡πÄ‡∏£‡∏≤"]):
                        content.append(text)
            else:
                # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ bullet points ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
                # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡∏´‡∏•‡∏±‡∏á h4 ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
                next_element = header.next_sibling
                if next_element:
                    text = next_element.strip()
                    if text:
                        content.append(text)
            
            # ‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
            if content:
                if matching_key in ["‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏°‡∏±‡∏ç", "‡∏ä‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏≤", "‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏¢‡∏≤"]:
                    data[matching_key] = clean_single_line_content(" ".join(content))
                else:
                    data[matching_key] = format_as_bullets(content)
        
        # ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ö‡∏≤‡∏á‡∏™‡πà‡∏ß‡∏ô ‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡∏ß‡∏¥‡∏ò‡∏µ‡∏≠‡∏∑‡πà‡∏ô
        if not any(data[key] for key in ["‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏°‡∏±‡∏ç", "‡∏ä‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏≤", "‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏¢‡∏≤"]):
            extract_from_html_elements(driver, data)
            
        # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
        for key, value in data.items():
            if isinstance(value, str) and value:
                cleaned = value.strip()
                if cleaned in ["", "-", "*", "**"] or len(cleaned) <= 3:
                    data[key] = ""
                else:
                    data[key] = cleaned

    except Exception as e:
        if DEBUG_MODE:
            debug_print(f"Error in extract_drug_details_fixed: {e}")

    return data

def extract_generic_name_comprehensive(driver, soup):
    """‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏°‡∏±‡∏ç‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°"""
    try:
        generic_name = ""
        
        # 1. ‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å div.bs-callout ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ "‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏°‡∏±‡∏ç"
        callouts = soup.find_all("div", class_="bs-callout")
        for callout in callouts:
            if "‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏°‡∏±‡∏ç" in callout.text:
                # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏±‡∏á‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ "‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏°‡∏±‡∏ç"
                text = callout.text.split("‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏°‡∏±‡∏ç")[-1].strip()
                if text and len(text) > 1:
                    return clean_single_line_content(text)
        
        # 2. ‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å h1, h2, h3, h4 ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ "‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏°‡∏±‡∏ç"
        for tag in ["h1", "h2", "h3", "h4"]:
            headers = soup.find_all(tag)
            for header in headers:
                if "‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏°‡∏±‡∏ç" in header.text:
                    # ‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
                    next_sibling = header.next_sibling
                    if next_sibling:
                        text = next_sibling.strip()
                        if text and len(text) > 1:
                            return clean_single_line_content(text)
        
        # 3. ‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å strong tag ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ "‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏°‡∏±‡∏ç"
        strongs = soup.find_all("strong")
        for strong in strongs:
            if "‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏°‡∏±‡∏ç" in strong.text:
                # ‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
                next_sibling = strong.next_sibling
                if next_sibling:
                    text = next_sibling.strip()
                    if text and len(text) > 1:
                        return clean_single_line_content(text)
        
        # 4. ‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å table ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ "‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏°‡∏±‡∏ç"
        tables = soup.find_all("table")
        for table in tables:
            rows = table.find_all("tr")
            for row in rows:
                cells = row.find_all("td")
                if len(cells) >= 2 and "‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏°‡∏±‡∏ç" in cells[0].text:
                    text = cells[1].text.strip()
                    if text and len(text) > 1:
                        return clean_single_line_content(text)
        
        # 5. ‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å meta tags
        meta_generic = soup.find("meta", {"name": "generic-name"}) or soup.find("meta", {"property": "og:generic-name"})
        if meta_generic and meta_generic.get("content"):
            text = meta_generic["content"].strip()
            if text and len(text) > 1:
                return clean_single_line_content(text)
        
        # 6. ‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å div ‡∏ó‡∏µ‡πà‡∏°‡∏µ id ‡∏´‡∏£‡∏∑‡∏≠ class ‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏°‡∏±‡∏ç
        generic_divs = soup.find_all("div", {"id": re.compile(r"generic.*name", re.I)}) + \
                      soup.find_all("div", {"class": re.compile(r"generic.*name", re.I)})
        for div in generic_divs:
            text = div.text.strip()
            if text and len(text) > 1:
                return clean_single_line_content(text)
        
        # 7. ‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å span ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ "‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏°‡∏±‡∏ç"
        spans = soup.find_all("span")
        for span in spans:
            if "‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏°‡∏±‡∏ç" in span.text:
                # ‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
                next_sibling = span.next_sibling
                if next_sibling:
                    text = next_sibling.strip()
                    if text and len(text) > 1:
                        return clean_single_line_content(text)
        
        return generic_name
        
    except Exception as e:
        if DEBUG_MODE:
            debug_print(f"Generic name extraction error: {e}")
        return ""

def extract_structured_data(driver):
    """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å structured data"""
    try:
        # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å schema.org ‡∏´‡∏£‡∏∑‡∏≠ JSON-LD
        scripts = driver.find_elements(By.XPATH, '//script[@type="application/ld+json"]')
        structured_data = {}
        
        for script in scripts:
            try:
                json_data = json.loads(script.get_attribute('innerHTML'))
                if isinstance(json_data, dict):
                    if json_data.get("@type") in ["Drug", "Medicine", "MedicalEntity"]:
                        structured_data.update({
                            "‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏°‡∏±‡∏ç": json_data.get("nonProprietaryName", ""),
                            "‡∏ä‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏≤": json_data.get("name", ""),
                            "‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏¢‡∏≤": json_data.get("dosageForm", ""),
                            "‡∏¢‡∏≤‡∏ô‡∏µ‡πâ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö": json_data.get("indication", "")
                        })
            except:
                continue
                
        return structured_data
    except:
        return {}

def extract_meta_data(driver):
    """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å meta tags"""
    try:
        meta_data = {}
        meta_tags = driver.find_elements(By.TAG_NAME, "meta")
        
        for tag in meta_tags:
            try:
                name = tag.get_attribute("name") or tag.get_attribute("property")
                content = tag.get_attribute("content")
                
                if name and content:
                    if "title" in name.lower():
                        meta_data["‡∏ä‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏≤"] = content
                    elif "description" in name.lower() and "‡∏¢‡∏≤‡∏ô‡∏µ‡πâ‡πÉ‡∏ä‡πâ" in content:
                        meta_data["‡∏¢‡∏≤‡∏ô‡∏µ‡πâ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö"] = content
            except:
                continue
                
        return meta_data
    except:
        return {}

def merge_structured_data(data, structured_data):
    """‡∏ú‡∏™‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å structured data"""
    for key, value in structured_data.items():
        if not data[key] and value:
            data[key] = value

def merge_meta_data(data, meta_data):
    """‡∏ú‡∏™‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å meta data"""
    for key, value in meta_data.items():
        if not data[key] and value:
            data[key] = value

def fill_missing_data(driver, data):
    """‡πÄ‡∏ï‡∏¥‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏´‡∏≤‡∏¢‡πÑ‡∏õ"""
    try:
        # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ä‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏≤ ‡∏•‡∏≠‡∏á‡∏´‡∏≤‡∏à‡∏≤‡∏Å‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏´‡∏ô‡πâ‡∏≤
        if not data["‡∏ä‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏≤"]:
            try:
                title = driver.title
                if title and ":" in title:
                    data["‡∏ä‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏≤"] = title.split(":")[0].strip()
            except:
                pass
        
        # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏¢‡∏≤ ‡∏•‡∏≠‡∏á‡∏´‡∏≤‡∏à‡∏≤‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        if not data["‡∏¢‡∏≤‡∏ô‡∏µ‡πâ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö"]:
            try:
                body_text = driver.find_element(By.TAG_NAME, "body").text
                usage_matches = re.findall(r"(?:‡∏¢‡∏≤‡∏ô‡∏µ‡πâ‡πÉ‡∏ä‡πâ|‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö|‡πÉ‡∏ä‡πâ‡∏£‡∏±‡∏Å‡∏©‡∏≤).*?(?=\n|$)", body_text)
                if usage_matches:
                    data["‡∏¢‡∏≤‡∏ô‡∏µ‡πâ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö"] = format_as_bullets(usage_matches)
            except:
                pass
                
        # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏¢‡∏≤ ‡∏•‡∏≠‡∏á‡∏´‡∏≤‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á
        if not data["‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏¢‡∏≤"]:
            try:
                body_text = driver.find_element(By.TAG_NAME, "body").text.lower()
                
                # ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏¢‡∏≤‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á
                form_patterns = {
                    "‡πÄ‡∏à‡∏•": [r"(?:‡∏¢‡∏≤)?‡πÄ‡∏à‡∏•", r"gel", r"‡πÄ‡∏à‡∏•‡∏ó‡∏≤", r"‡πÄ‡∏à‡∏•‡πÉ‡∏ä‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡∏µ‡πà", r"(?:‡∏¢‡∏≤)?‡πÄ‡∏à‡∏•\s*(?:‡∏ó‡∏≤)?", r"gel\s*(?:form)?", r"topical gel"],
                    "‡πÄ‡∏°‡πá‡∏î": [r"‡∏¢‡∏≤‡πÄ‡∏°‡πá‡∏î", r"tablet", r"‡πÄ‡∏°‡πá‡∏î"],
                    "‡πÅ‡∏Ñ‡∏õ‡∏ã‡∏π‡∏•": [r"‡πÅ‡∏Ñ‡∏õ‡∏ã‡∏π‡∏•", r"capsule"],
                    "‡∏ô‡πâ‡∏≥": [r"‡∏¢‡∏≤‡∏ô‡πâ‡∏≥", r"‡∏ô‡πâ‡∏≥‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°", r"‡∏ô‡πâ‡∏≥‡πÅ‡∏Ç‡∏ß‡∏ô‡∏ï‡∏∞‡∏Å‡∏≠‡∏ô", r"syrup", r"suspension"],
                    "‡∏Ñ‡∏£‡∏µ‡∏°": [r"‡∏Ñ‡∏£‡∏µ‡∏°", r"cream"],
                    "‡∏Ç‡∏µ‡πâ‡∏ú‡∏∂‡πâ‡∏á": [r"‡∏Ç‡∏µ‡πâ‡∏ú‡∏∂‡πâ‡∏á", r"ointment"],
                    "‡∏¢‡∏≤‡∏â‡∏µ‡∏î": [r"‡∏â‡∏µ‡∏î", r"injection"],
                    "‡∏¢‡∏≤‡∏û‡πà‡∏ô": [r"‡∏û‡πà‡∏ô", r"spray"]
                }
                
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏¢‡∏≤
                for form, patterns in form_patterns.items():
                    for pattern in patterns:
                        if re.search(pattern, body_text, re.IGNORECASE):
                            data["‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏¢‡∏≤"] = form
                            return
                            
                # ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏û‡∏ö ‡∏•‡∏≠‡∏á‡∏´‡∏≤‡∏à‡∏≤‡∏Å class ‡∏´‡∏£‡∏∑‡∏≠ id ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á
                form_elements = driver.find_elements(By.CSS_SELECTOR, "[class*='form'], [class*='type'], [id*='form'], [id*='type']")
                for element in form_elements:
                    element_text = element.text.lower()
                    for form, patterns in form_patterns.items():
                        if any(re.search(pattern, element_text, re.IGNORECASE) for pattern in patterns):
                            data["‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏¢‡∏≤"] = form
                            return
            except:
                pass
    except:
        pass

def extract_from_html_elements(driver, data):
    """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å HTML elements ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á"""
    try:
        # 1. ‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å table
        tables = driver.find_elements(By.TAG_NAME, "table")
        for table in tables:
            rows = table.find_elements(By.TAG_NAME, "tr")
            for row in rows:
                cells = row.find_elements(By.TAG_NAME, "td")
                if len(cells) >= 2:
                    header = cells[0].text.strip()
                    content = cells[1].text.strip()
                    
                    for section in data.keys():
                        if section != "URL" and not data[section] and section in header and content:
                            data[section] = clean_single_line_content(content) if section in ["‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏°‡∏±‡∏ç", "‡∏ä‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏≤", "‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏¢‡∏≤"] else format_as_bullets([content])
        
        # 2. ‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å div ‡∏ó‡∏µ‡πà‡∏°‡∏µ class ‡∏´‡∏£‡∏∑‡∏≠ id ‡πÄ‡∏â‡∏û‡∏≤‡∏∞
        target_classes = ["drug-info", "drug-details", "drug-content", "content", "main"]
        for class_name in target_classes:
            elements = driver.find_elements(By.CLASS_NAME, class_name)
            for element in elements:
                extract_from_element_content(element, data)
        
        # 3. ‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å heading elements
        headings = []
        for level in range(1, 7):
            headings.extend(driver.find_elements(By.TAG_NAME, f"h{level}"))
        
        for heading in headings:
            try:
                heading_text = heading.text.strip()
                for section in data.keys():
                    if section != "URL" and not data[section] and section in heading_text:
                        # ‡∏´‡∏≤‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏à‡∏≤‡∏Å element ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
                        next_element = get_next_sibling_element(driver, heading)
                        if next_element:
                            content = next_element.text.strip()
                            if content:
                                data[section] = clean_single_line_content(content) if section in ["‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏°‡∏±‡∏ç", "‡∏ä‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏≤", "‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏¢‡∏≤"] else format_as_bullets([content])
            except:
                continue
        
        # 4. ‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å list elements
        lists = driver.find_elements(By.TAG_NAME, "ul") + driver.find_elements(By.TAG_NAME, "ol")
        for list_element in lists:
            try:
                # ‡∏´‡∏≤ heading ‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤
                prev_element = get_previous_sibling_element(driver, list_element)
                if prev_element:
                    prev_text = prev_element.text.strip()
                    for section in data.keys():
                        if section != "URL" and not data[section] and section in prev_text:
                            list_items = list_element.find_elements(By.TAG_NAME, "li")
                            content = [item.text.strip() for item in list_items if item.text.strip()]
                            if content:
                                data[section] = format_as_bullets(content)
            except:
                continue
        
        # 5. ‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å structured elements
        sections = driver.find_elements(By.TAG_NAME, "section")
        for section_element in sections:
            try:
                section_id = section_element.get_attribute("id") or ""
                section_class = section_element.get_attribute("class") or ""
                
                for section in data.keys():
                    if section != "URL" and not data[section]:
                        section_lower = section.lower()
                        if (section_lower in section_id.lower() or 
                            section_lower in section_class.lower() or 
                            section in section_element.text):
                            content = section_element.text.strip()
                            if content:
                                data[section] = clean_single_line_content(content) if section in ["‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏°‡∏±‡∏ç", "‡∏ä‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏≤", "‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏¢‡∏≤"] else format_as_bullets([content])
            except:
                continue
                
    except Exception as e:
        debug_print(f"HTML element extraction failed: {e}")

def extract_from_element_content(element, data):
    """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏Ç‡∏≠‡∏á element"""
    try:
        element_text = element.text.strip()
        if not element_text:
            return
            
        # ‡πÅ‡∏¢‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î
        lines = [line.strip() for line in element_text.split('\n') if line.strip()]
        
        for i, line in enumerate(lines):
            for section in data.keys():
                if section != "URL" and not data[section] and section in line:
                    # ‡∏´‡∏≤‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÉ‡∏ô‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
                    content_lines = []
                    j = i + 1
                    while j < len(lines) and not any(key in lines[j] for key in data.keys() if key != "URL"):
                        content_lines.append(lines[j])
                        j += 1
                    
                    if content_lines:
                        content = "\n".join(content_lines)
                        data[section] = clean_single_line_content(content) if section in ["‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏°‡∏±‡∏ç", "‡∏ä‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏≤", "‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏¢‡∏≤"] else format_as_bullets(content_lines)
    except:
        pass

def get_next_sibling_element(driver, element):
    """‡∏´‡∏≤ element ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ"""
    try:
        return driver.execute_script("""
            function getNextSibling(elem) {
                var sibling = elem.nextElementSibling;
                while (sibling) {
                    if (sibling.offsetParent !== null) {  // check if element is visible
                        return sibling;
                    }
                    sibling = sibling.nextElementSibling;
                }
                return null;
            }
            return getNextSibling(arguments[0]);
        """, element)
    except:
        return None

def get_previous_sibling_element(driver, element):
    """‡∏´‡∏≤ element ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤"""
    try:
        return driver.execute_script("""
            function getPreviousSibling(elem) {
                var sibling = elem.previousElementSibling;
                while (sibling) {
                    if (sibling.offsetParent !== null) {  // check if element is visible
                        return sibling;
                    }
                    sibling = sibling.previousElementSibling;
                }
                return null;
            }
            return getPreviousSibling(arguments[0]);
        """, element)
    except:
        return None

def safe_click_element(driver, element, max_retries=3):
    """‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡∏•‡∏¥‡∏Å element"""
    for retry in range(max_retries):
        try:
            current_url = driver.current_url
            driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", element)
            time.sleep(random.uniform(0.5, 1.0))
            
            driver.execute_script("arguments[0].click();", element)
            time.sleep(random.uniform(1.5, 2.5))
            
            if driver.current_url != current_url:
                return True
            else:
                element.click()
                time.sleep(random.uniform(1.5, 2.5))
                if driver.current_url != current_url:
                    return True
                    
        except StaleElementReferenceException:
            time.sleep(1)
            return False
        except Exception as e:
            debug_print(f"Click failed, retry {retry + 1}: {e}")
            time.sleep(random.uniform(1, 2))
    
    return False

def safe_navigate_back(driver, max_retries=3):
    """‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Å‡∏≤‡∏£‡∏¢‡πâ‡∏≠‡∏ô‡∏Å‡∏•‡∏±‡∏ö"""
    for retry in range(max_retries):
        try:
            current_url = driver.current_url
            driver.back()
            time.sleep(random.uniform(2, 4))
            
            WebDriverWait(driver, 15).until(lambda d: d.execute_script("return document.readyState") == "complete")
            
            if driver.current_url != current_url:
                return True
                
        except Exception as e:
            debug_print(f"Navigate back failed, retry {retry + 1}: {e}")
            time.sleep(random.uniform(1, 2))
    
    # Fallback
    try:
        driver.get("https://www.yaandyou.net/")
        time.sleep(random.uniform(3, 5))
        return True
    except:
        return False

def wait_for_page_load(driver, timeout=15):
    """‡∏£‡∏≠‡πÉ‡∏´‡πâ‡∏´‡∏ô‡πâ‡∏≤‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏™‡∏£‡πá‡∏à"""
    try:
        WebDriverWait(driver, timeout).until(lambda d: d.execute_script("return document.readyState") == "complete")
        time.sleep(random.uniform(0.5, 1.5))
        return True
    except TimeoutException:
        return False

def save_to_csv(data, filename):
    """‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡∏á CSV"""
    fieldnames = [
        "‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏°‡∏±‡∏ç", "‡∏ä‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏≤", "‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏¢‡∏≤", "‡∏¢‡∏≤‡∏ô‡∏µ‡πâ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö", "‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏¢‡∏≤",
        "‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏£‡πÅ‡∏à‡πâ‡∏á‡πÉ‡∏´‡πâ‡πÅ‡∏û‡∏ó‡∏¢‡πå‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏†‡∏™‡∏±‡∏ä‡∏Å‡∏£‡∏ó‡∏£‡∏≤‡∏ö", "‡∏ó‡∏≥‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£‡∏´‡∏≤‡∏Å‡∏•‡∏∑‡∏°‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏ó‡∏≤‡∏ô‡∏¢‡∏≤‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏ä‡πâ‡∏¢‡∏≤",
        "‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡πÑ‡∏°‡πà‡∏û‡∏∂‡∏á‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ", "‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡πÑ‡∏°‡πà‡∏û‡∏∂‡∏á‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏à‡πâ‡∏á‡πÅ‡∏û‡∏ó‡∏¢‡πå‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏†‡∏™‡∏±‡∏ä‡∏Å‡∏£‡∏ó‡∏±‡∏ô‡∏ó‡∏µ",
        "‡∏Å‡∏≤‡∏£‡πÄ‡∏Å‡πá‡∏ö‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏¢‡∏≤", "URL"
    ]

    try:
        os.makedirs(os.path.dirname(filename) if os.path.dirname(filename) else '.', exist_ok=True)
        
        with open(filename, "w", encoding="utf-8-sig", newline="") as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            for d in data:
                row = {k: d.get(k, "").strip().replace('\n\n', '\n').replace('\r', '') for k in fieldnames}
                writer.writerow(row)
        
        logger.info(f"‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå {filename} ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à ({len(data)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£)")
        return True
    except Exception as e:
        logger.error(f"Error saving CSV {filename}: {e}")
        return False
    
def save_to_json(data, filename):
    """‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡∏á JSON"""
    try:
        os.makedirs(os.path.dirname(filename) if os.path.dirname(filename) else '.', exist_ok=True)
        
        with open(filename, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå JSON {filename} ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à ({len(data)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£)")
        return True
    except Exception as e:
        logger.error(f"Error saving JSON {filename}: {e}")
        return False

def save_to_txt(data, filename):
    """‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡∏á TXT ‡πÅ‡∏ö‡∏ö‡∏≠‡πà‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢"""
    try:
        os.makedirs(os.path.dirname(filename) if os.path.dirname(filename) else '.', exist_ok=True)
        
        with open(filename, "w", encoding="utf-8") as f:
            f.write("=" * 100 + "\n")
            f.write(f"üìä ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏¢‡∏≤ - ‡∏£‡∏ß‡∏° {len(data)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n")
            f.write("=" * 100 + "\n\n")
            
            for i, drug in enumerate(data, 1):
                f.write(f"üíä ‡∏¢‡∏≤‡∏ó‡∏µ‡πà {i}\n")
                f.write("-" * 80 + "\n")
                
                # ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
                basic_info = ["‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏°‡∏±‡∏ç", "‡∏ä‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏≤", "‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏¢‡∏≤"]
                for key in basic_info:
                    value = drug.get(key, "")
                    status = "üîπ" if value else "‚ùå"
                    f.write(f"{status} {key}: {value if value else '(‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•)'}\n")
                
                f.write("-" * 80 + "\n")
                
                # ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î
                detail_keys = [
                    "‡∏¢‡∏≤‡∏ô‡∏µ‡πâ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö", "‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏¢‡∏≤", "‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏£‡πÅ‡∏à‡πâ‡∏á‡πÉ‡∏´‡πâ‡πÅ‡∏û‡∏ó‡∏¢‡πå‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏†‡∏™‡∏±‡∏ä‡∏Å‡∏£‡∏ó‡∏£‡∏≤‡∏ö",
                    "‡∏ó‡∏≥‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£‡∏´‡∏≤‡∏Å‡∏•‡∏∑‡∏°‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏ó‡∏≤‡∏ô‡∏¢‡∏≤‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏ä‡πâ‡∏¢‡∏≤", "‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡πÑ‡∏°‡πà‡∏û‡∏∂‡∏á‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ",
                    "‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡πÑ‡∏°‡πà‡∏û‡∏∂‡∏á‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏à‡πâ‡∏á‡πÅ‡∏û‡∏ó‡∏¢‡πå‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏†‡∏™‡∏±‡∏ä‡∏Å‡∏£‡∏ó‡∏±‡∏ô‡∏ó‡∏µ", "‡∏Å‡∏≤‡∏£‡πÄ‡∏Å‡πá‡∏ö‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏¢‡∏≤"
                ]
                
                for key in detail_keys:
                    value = drug.get(key, "")
                    f.write(f"\nüî∏ {key}:\n")
                    if value:
                        for line in value.split('\n'):
                            if line.strip():
                                f.write(f"   {line.strip()}\n")
                    else:
                        f.write("   (‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•)\n")
                
                if drug.get("URL"):
                    f.write(f"\nüîó URL: {drug['URL']}\n")
                f.write("=" * 100 + "\n\n")
        
        logger.info(f"‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå TXT {filename} ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à ({len(data)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£)")
        return True
    except Exception as e:
        logger.error(f"Error saving TXT {filename}: {e}")
        return False

def save_to_excel(data, filename):
    """‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡∏á Excel (‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á openpyxl ‡∏Å‡πà‡∏≠‡∏ô)"""
    try:
        import openpyxl
        from openpyxl.styles import Font, PatternFill, Alignment
        from openpyxl.utils.dataframe import dataframe_to_rows
        import pandas as pd
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á DataFrame
        df = pd.DataFrame(data)
        
        # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå
        column_order = [
            "‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏°‡∏±‡∏ç", "‡∏ä‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏≤", "‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏¢‡∏≤", "‡∏¢‡∏≤‡∏ô‡∏µ‡πâ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö", "‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏¢‡∏≤",
            "‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏£‡πÅ‡∏à‡πâ‡∏á‡πÉ‡∏´‡πâ‡πÅ‡∏û‡∏ó‡∏¢‡πå‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏†‡∏™‡∏±‡∏ä‡∏Å‡∏£‡∏ó‡∏£‡∏≤‡∏ö", "‡∏ó‡∏≥‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£‡∏´‡∏≤‡∏Å‡∏•‡∏∑‡∏°‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏ó‡∏≤‡∏ô‡∏¢‡∏≤‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏ä‡πâ‡∏¢‡∏≤",
            "‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡πÑ‡∏°‡πà‡∏û‡∏∂‡∏á‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ", "‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡πÑ‡∏°‡πà‡∏û‡∏∂‡∏á‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏à‡πâ‡∏á‡πÅ‡∏û‡∏ó‡∏¢‡πå‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏†‡∏™‡∏±‡∏ä‡∏Å‡∏£‡∏ó‡∏±‡∏ô‡∏ó‡∏µ",
            "‡∏Å‡∏≤‡∏£‡πÄ‡∏Å‡πá‡∏ö‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏¢‡∏≤", "URL"
        ]
        df = df.reindex(columns=column_order)
        
        os.makedirs(os.path.dirname(filename) if os.path.dirname(filename) else '.', exist_ok=True)
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á workbook
        wb = openpyxl.Workbook()
        ws = wb.active
        ws.title = "Drug Information"
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡∏á worksheet
        for r in dataframe_to_rows(df, index=False, header=True):
            ws.append(r)
        
        # ‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏´‡∏±‡∏ß‡∏ï‡∏≤‡∏£‡∏≤‡∏á
        header_font = Font(bold=True, color="FFFFFF")
        header_fill = PatternFill(start_color="4472C4", end_color="4472C4", fill_type="solid")
        
        for cell in ws[1]:
            cell.font = header_font
            cell.fill = header_fill
            cell.alignment = Alignment(horizontal="center", vertical="center")
        
        # ‡∏õ‡∏£‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡∏ß‡πâ‡∏≤‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå
        column_widths = {
            'A': 20, 'B': 25, 'C': 15, 'D': 40, 'E': 30,
            'F': 35, 'G': 30, 'H': 25, 'I': 35, 'J': 20, 'K': 50
        }
        
        for col, width in column_widths.items():
            ws.column_dimensions[col].width = width
        
        # Wrap text ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß
        for row in ws.iter_rows(min_row=2, max_row=ws.max_row):
            for cell in row:
                cell.alignment = Alignment(wrap_text=True, vertical="top")
        
        wb.save(filename)
        logger.info(f"‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå Excel {filename} ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à ({len(data)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£)")
        return True
        
    except ImportError:
        logger.warning("‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Excel ‡πÑ‡∏î‡πâ ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á: pip install openpyxl pandas")
        return False
    except Exception as e:
        logger.error(f"Error saving Excel {filename}: {e}")
        return False

def save_all_formats(data, base_filename):
    """‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏∏‡∏Å‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö"""
    base_name = base_filename.replace('.csv', '')
    
    results = {
        'csv': save_to_csv(data, f"{base_name}.csv"),
        'json': save_to_json(data, f"{base_name}.json"),
        'txt': save_to_txt(data, f"{base_name}.txt"),
        'excel': save_to_excel(data, f"{base_name}.xlsx")
    }
    
    success_count = sum(results.values())
    logger.info(f"üìÅ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à {success_count}/4 ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö")
    
    return results

def load_existing_data(filename):
    """‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß"""
    if not os.path.exists(filename):
        return []
    
    try:
        with open(filename, "r", encoding="utf-8-sig") as f:
            data = list(csv.DictReader(f))
            logger.info(f"‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏î‡∏¥‡∏°: {len(data)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏à‡∏≤‡∏Å {filename}")
            return data
    except Exception as e:
        logger.warning(f"‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏î‡∏¥‡∏°: {e}")
        return []

def is_duplicate_drug(new_drug, existing_drugs):
    """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏¢‡∏≤‡∏ã‡πâ‡∏≥‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà"""
    trade_name = new_drug.get("‡∏ä‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏≤", "").strip()
    generic_name = new_drug.get("‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏°‡∏±‡∏ç", "").strip()
    url = new_drug.get("URL", "").strip()
    
    if not trade_name and not generic_name:
        return True
    
    def clean_name(name):
        """‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏ä‡∏∑‡πà‡∏≠‡∏¢‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö"""
        if not name:
            return ""
        # ‡∏•‡∏ö‡∏ß‡∏á‡πÄ‡∏•‡πá‡∏ö‡πÅ‡∏•‡∏∞‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏ô‡∏ß‡∏á‡πÄ‡∏•‡πá‡∏ö
        name = re.sub(r'\([^)]*\)', '', name)
        # ‡∏•‡∏ö‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏û‡∏¥‡πÄ‡∏®‡∏©
        name = re.sub(r'[^\w\s]', '', name)
        # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏û‡∏¥‡∏°‡∏û‡πå‡πÄ‡∏•‡πá‡∏Å‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡∏î‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á
        return ' '.join(name.lower().split())
    
    clean_trade = clean_name(trade_name)
    clean_generic = clean_name(generic_name)
    
    for existing in existing_drugs:
        existing_trade = clean_name(existing.get("‡∏ä‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏≤", ""))
        existing_generic = clean_name(existing.get("‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏°‡∏±‡∏ç", ""))
        existing_url = existing.get("URL", "").strip()
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö URL ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
        if url and existing_url and url == existing_url:
            return True
            
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏≤
        if clean_trade and existing_trade:
            if clean_trade == existing_trade:
                return True
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏™‡∏∞‡∏Å‡∏î‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢
            if len(clean_trade) > 3 and (clean_trade in existing_trade or existing_trade in clean_trade):
                return True
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏°‡∏±‡∏ç
        if clean_generic and existing_generic:
            if clean_generic == existing_generic:
                return True
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏™‡∏∞‡∏Å‡∏î‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢
            if len(clean_generic) > 3 and (clean_generic in existing_generic or existing_generic in clean_generic):
                return True
    
    return False

def setup_driver():
    """‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Chrome driver"""
    try:
        options = webdriver.ChromeOptions()
        options.add_experimental_option("detach", True)
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--disable-blink-features=AutomationControlled")
        options.add_argument("--disable-images")
        options.add_experimental_option("excludeSwitches", ["enable-automation"])
        options.add_experimental_option('useAutomationExtension', False)
        options.add_argument("--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36")
        
        prefs = {
            "profile.managed_default_content_settings.images": 2,
            "profile.default_content_setting_values.notifications": 2,
            "profile.default_content_settings.popups": 0
        }
        options.add_experimental_option("prefs", prefs)
        
        try:
            service = Service(ChromeDriverManager().install())
        except Exception as e:
            logger.error(f"Error installing ChromeDriver: {e}")
            # Try using the default Chrome driver path as fallback
            service = Service("chromedriver")
            
        driver = webdriver.Chrome(service=service, options=options)
        return driver
    except Exception as e:
        logger.error(f"Error setting up Chrome driver: {e}")
        raise

def load_processed_keywords():
    """‡πÇ‡∏´‡∏•‡∏î‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ keywords ‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏õ‡πÅ‡∏•‡πâ‡∏ß"""
    try:
        if os.path.exists("processed_keywords.txt"):
            with open("processed_keywords.txt", "r", encoding="utf-8") as f:
                return set(line.strip() for line in f if line.strip())
        return set()
    except Exception as e:
        logger.warning(f"‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÇ‡∏´‡∏•‡∏î processed keywords: {e}")
        return set()

def save_processed_keywords(keywords):
    """‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ keywords ‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏õ‡πÅ‡∏•‡πâ‡∏ß"""
    try:
        with open("processed_keywords.txt", "w", encoding="utf-8") as f:
            for keyword in sorted(keywords):
                f.write(f"{keyword}\n")
        logger.info(f"‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å processed keywords: {len(keywords)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
    except Exception as e:
        logger.error(f"‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å processed keywords: {e}")

def main():
    """‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å"""
    driver = None
    processed_keywords = load_processed_keywords()
    
    try:
        driver = setup_driver()
        driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        driver.set_page_load_timeout(30)
        
        logger.info("üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô...")
        driver.get("https://www.yaandyou.net/")
        wait_for_page_load(driver)

        # ‡∏Ç‡πâ‡∏≤‡∏° SSL warning
        try:
            driver.find_element(By.ID, "details-button").click()
            time.sleep(0.5)
            driver.find_element(By.ID, "proceed-link").click()
            time.sleep(2)
        except NoSuchElementException:
            pass

        all_drugs = load_existing_data("drug_backup.csv")
        keywords = generate_keywords()
        
        # ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞ keywords ‡∏ó‡∏µ‡πà‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•
        remaining_keywords = [k for k in keywords if k not in processed_keywords]
        
        logger.info(f"üìä ‡∏à‡∏∞‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• {len(remaining_keywords)} keywords ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠")
        logger.info(f"üìö ‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏î‡∏¥‡∏° {len(all_drugs)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
        logger.info(f"‚úÖ ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏õ‡πÅ‡∏•‡πâ‡∏ß {len(processed_keywords)} keywords")

        for idx, keyword in enumerate(remaining_keywords):
            if keyword in processed_keywords:
                continue
                
            logger.info(f"üîç ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤: {keyword} ({idx+1}/{len(remaining_keywords)}) - ‡∏£‡∏ß‡∏°: {len(all_drugs)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
            
            try:
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏´‡∏•‡∏±‡∏Å
                if "yaandyou.net" not in driver.current_url or "index.php" not in driver.current_url:
                    driver.get("https://www.yaandyou.net/")
                    wait_for_page_load(driver)
                
                # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ keyword
                input_box = WebDriverWait(driver, 15).until(EC.element_to_be_clickable((By.ID, "keyword")))
                input_box.clear()
                input_box.send_keys(keyword)
                time.sleep(random.uniform(1.5, 2.5))

                # ‡∏£‡∏≠ autocomplete
                try:
                    WebDriverWait(driver, 8).until(EC.presence_of_element_located((By.XPATH, '//*[@id="results"]/div')))
                    suggestions = driver.find_elements(By.XPATH, '//*[@id="results"]/div')
                except TimeoutException:
                    logger.info(f"‡πÑ‡∏°‡πà‡∏°‡∏µ autocomplete ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö {keyword}")
                    processed_keywords.add(keyword)
                    continue

                logger.info(f"‡∏û‡∏ö autocomplete: {len(suggestions)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")

                # ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏ï‡πà‡∏•‡∏∞ suggestion
                for i in range(len(suggestions)):
                    try:
                        # ‡∏£‡∏µ‡πÄ‡∏ü‡∏£‡∏ä suggestions
                        input_box = WebDriverWait(driver, 15).until(EC.element_to_be_clickable((By.ID, "keyword")))
                        input_box.clear()
                        input_box.send_keys(keyword)
                        time.sleep(random.uniform(1.5, 2.5))

                        try:
                            WebDriverWait(driver, 8).until(EC.presence_of_element_located((By.XPATH, '//*[@id="results"]/div')))
                            suggestions = driver.find_elements(By.XPATH, '//*[@id="results"]/div')
                        except TimeoutException:
                            break
                            
                        if i >= len(suggestions):
                            continue

                        if not safe_click_element(driver, suggestions[i]):
                            continue
                            
                        wait_for_page_load(driver)

                        if "index_list.php" not in driver.current_url:
                            safe_navigate_back(driver)
                            continue

                        # ‡∏´‡∏≤‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏¢‡∏≤
                        rows = []
                        for selector in ['#tableId3 tbody tr', 'table tbody tr', '.table tbody tr', 'tbody tr']:
                            try:
                                WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, selector)))
                                all_rows = driver.find_elements(By.CSS_SELECTOR, selector)
                                rows = [row for row in all_rows if row.find_elements(By.TAG_NAME, "td")]
                                if rows:
                                    break
                            except TimeoutException:
                                continue

                        if not rows:
                            safe_navigate_back(driver)
                            continue

                        logger.info(f" ‚Üí ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏¢‡∏≤: {len(rows)}")

                        # ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÅ‡∏ñ‡∏ß‡∏¢‡∏≤
                        for j in range(len(rows)):
                            try:
                                # ‡∏£‡∏µ‡πÄ‡∏ü‡∏£‡∏ä rows
                                fresh_rows = []
                                for selector in ['#tableId3 tbody tr', 'table tbody tr', '.table tbody tr', 'tbody tr']:
                                    try:
                                        all_rows = driver.find_elements(By.CSS_SELECTOR, selector)
                                        fresh_rows = [row for row in all_rows if row.find_elements(By.TAG_NAME, "td")]
                                        if fresh_rows:
                                            break
                                    except:
                                        continue

                                if j >= len(fresh_rows):
                                    break

                                # ‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏≤
                                row_cells = fresh_rows[j].find_elements(By.TAG_NAME, "td")
                                trade_name = ""
                                for cell in row_cells:
                                    cell_text = cell.text.strip()
                                    if cell_text and len(cell_text) > 1:
                                        trade_name = cell_text
                                        break

                                if not trade_name or is_duplicate_drug({"‡∏ä‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏≤": trade_name}, all_drugs):
                                    continue

                                if not safe_click_element(driver, fresh_rows[j]):
                                    continue
                                    
                                wait_for_page_load(driver)

                                # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏¢‡∏≤
                                data = extract_drug_details_fixed(driver)
                                
                                if not data["‡∏ä‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏≤"]:
                                    data["‡∏ä‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏≤"] = trade_name
                                
                                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
                                if ((data["‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏°‡∏±‡∏ç"] or data["‡∏ä‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏≤"]) and 
                                    (data["‡∏¢‡∏≤‡∏ô‡∏µ‡πâ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö"] or data["‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏¢‡∏≤"])):
                                    
                                    all_drugs.append(data)
                                    print_drug_data(data, len(all_drugs))
                                    logger.info(f"  ‚úîÔ∏è {trade_name} (‡∏£‡∏ß‡∏°: {len(all_drugs)})")

                                # ‡∏Å‡∏•‡∏±‡∏ö‡∏´‡∏ô‡πâ‡∏≤ index_list.php
                                safe_navigate_back(driver)
                                wait_for_page_load(driver)

                                time.sleep(random.uniform(0.5, 1.5))

                            except Exception as e:
                                logger.error(f"Error processing drug row {j}: {e}")
                                continue

                        # ‡∏Å‡∏•‡∏±‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏´‡∏•‡∏±‡∏Å
                        safe_navigate_back(driver)

                    except Exception as e:
                        logger.error(f"Error processing suggestion {i}: {e}")
                        continue

                processed_keywords.add(keyword)
                
                # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å processed keywords ‡∏ó‡∏∏‡∏Å 5 keywords
                if len(processed_keywords) % 5 == 0:
                    save_processed_keywords(processed_keywords)

            except Exception as e:
                logger.error(f"Keyword error for {keyword}: {e}")
                continue

            # ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å backup
            print_summary_stats(all_drugs, keyword, idx + 1)
            if len(all_drugs) > 0 and len(all_drugs) % 20 == 0:
                save_all_formats(all_drugs, "drug_backup.csv")
                save_processed_keywords(processed_keywords)
                logger.info(f"üìù ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• backup: {len(all_drugs)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")

            time.sleep(random.uniform(1, 3))

        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
        save_all_formats(all_drugs, "drug_full_details.csv")
        save_all_formats(all_drugs, "drug_backup.csv")
        save_processed_keywords(processed_keywords)
        logger.info(f"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {len(all_drugs)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß")

    except KeyboardInterrupt:
        logger.info("\n‚ö†Ô∏è ‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏´‡∏¢‡∏∏‡∏î‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô...")
        if 'all_drugs' in locals() and all_drugs:
            save_to_csv(all_drugs, "drug_interrupted_backup.csv")
        if 'processed_keywords' in locals():
            save_processed_keywords(processed_keywords)
    
    except Exception as e:
        logger.error(f"Critical error: {e}")
        if 'all_drugs' in locals() and all_drugs:
            save_to_csv(all_drugs, "drug_emergency_backup.csv")
        if 'processed_keywords' in locals():
            save_processed_keywords(processed_keywords)
    
    finally:
        if driver:
            try:
                driver.quit()
                logger.info("üîö ‡∏õ‡∏¥‡∏î browser ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß")
            except:
                pass

def test_mode(mode, *args):
    """‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏£‡∏ß‡∏°"""
    options = setup_driver()
    driver = None
    
    try:
        driver = webdriver.Chrome(options=options)
        driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        
        if mode == "url" and args:
            # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö URL ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
            logger.info(f"üß™ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö URL: {args[0]}")
            driver.get(args[0])
            wait_for_page_load(driver)
            
            data = extract_drug_details_fixed(driver)
            print_drug_data(data)
            
            # ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥
            total_fields = len([k for k in data.keys() if k != "URL"])
            filled_fields = len([k for k, v in data.items() if k != "URL" and v.strip()])
            print(f"\nüìä ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {filled_fields}/{total_fields} ‡∏ü‡∏¥‡∏•‡∏î‡πå ({(filled_fields/total_fields)*100:.1f}%)")
            
            return data
            
        elif mode == "quick":
            # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÄ‡∏£‡πá‡∏ß
            logger.info("‚ö° ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÄ‡∏£‡πá‡∏ß")
            driver.get("https://www.yaandyou.net/")
            wait_for_page_load(driver)

            # ‡∏Ç‡πâ‡∏≤‡∏° SSL warning
            try:
                driver.find_element(By.ID, "details-button").click()
                time.sleep(0.5)
                driver.find_element(By.ID, "proceed-link").click()
                time.sleep(2)
            except NoSuchElementException:
                pass

            # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ keyword
            keyword = args[0] if args else "ab"
            input_box = WebDriverWait(driver, 15).until(EC.element_to_be_clickable((By.ID, "keyword")))
            input_box.clear()
            input_box.send_keys(keyword)
            time.sleep(2)

            # ‡∏Ñ‡∏•‡∏¥‡∏Å suggestion ‡πÅ‡∏•‡∏∞‡∏¢‡∏≤‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å
            try:
                WebDriverWait(driver, 8).until(EC.presence_of_element_located((By.XPATH, '//*[@id="results"]/div')))
                suggestions = driver.find_elements(By.XPATH, '//*[@id="results"]/div')
                
                if suggestions:
                    safe_click_element(driver, suggestions[0])
                    wait_for_page_load(driver)
                    
                    for selector in ['#tableId3 tbody tr', 'table tbody tr']:
                        try:
                            rows = driver.find_elements(By.CSS_SELECTOR, selector)
                            if rows:
                                safe_click_element(driver, rows[0])
                                wait_for_page_load(driver)
                                
                                # ‡πÄ‡∏õ‡∏¥‡∏î debug ‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
                                if DEBUG_MODE:
                                    debug_print("=== Quick Test Debug ===")
                                    page_text = driver.find_element(By.TAG_NAME, "body").text
                                    lines = [line.strip() for line in page_text.split('\n') if line.strip()]
                                    usage_line = next((i for i, line in enumerate(lines) if "‡∏¢‡∏≤‡∏ô‡∏µ‡πâ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö" in line), None)
                                    if usage_line:
                                        debug_print(f"Found '‡∏¢‡∏≤‡∏ô‡∏µ‡πâ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö' at line {usage_line + 1}")
                                
                                data = extract_drug_details_fixed(driver)
                                print_drug_data(data)
                                
                                # ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏• '‡∏¢‡∏≤‡∏ô‡∏µ‡πâ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö'
                                usage_data = data.get("‡∏¢‡∏≤‡∏ô‡∏µ‡πâ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö", "")
                                if usage_data:
                                    bullets = usage_data.split('\n')
                                    print(f"\nüéØ '‡∏¢‡∏≤‡∏ô‡∏µ‡πâ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö': {len(bullets)} bullet points")
                                    if len(bullets) == 3:
                                        print("‚úÖ ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô bullet points ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á")
                                    else:
                                        print("‚ùå ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô bullet points ‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á (‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ 3)")
                                else:
                                    print("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• '‡∏¢‡∏≤‡∏ô‡∏µ‡πâ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö'")
                                
                                return data
                            break
                        except:
                            continue
                            
            except TimeoutException:
                print(f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö autocomplete ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö {keyword}")

    except Exception as e:
        logger.error(f"Test error: {e}")
    finally:
        if driver:
            driver.quit()
    
    return None

def extract_usage_bullets_comprehensive(text):
    """‡∏î‡∏∂‡∏á bullet points ‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏¢‡∏≤‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°"""
    try:
        bullets = []
        seen = set()
        
        # 1. ‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å HTML bullet points
        html_bullets = re.findall(r'<li>(.*?)</li>', text, re.DOTALL)
        if html_bullets:
            for bullet in html_bullets:
                clean_bullet = clean_bullet_text(bullet)
                if clean_bullet and clean_bullet not in seen:
                    bullets.append(clean_bullet)
                    seen.add(clean_bullet)
        
        # 2. ‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢ bullet
        if not bullets:
            bullet_markers = ['‚Ä¢', '¬∑', '*', '-', '‚óã', '‚óè', '‚ñ†', '‚ñ™', '‚ó¶']
            for marker in bullet_markers:
                if marker in text:
                    marker_bullets = [b.strip() for b in text.split(marker) if b.strip()]
                    for bullet in marker_bullets:
                        clean_bullet = clean_bullet_text(bullet)
                        if clean_bullet and clean_bullet not in seen:
                            bullets.append(clean_bullet)
                            seen.add(clean_bullet)
                    if bullets:
                        break
        
        # 3. ‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á
        if not bullets:
            sentences = re.split(r'[.!?]', text)
            usage_keywords = [
                "‡∏¢‡∏≤‡∏ô‡∏µ‡πâ‡πÉ‡∏ä‡πâ", "‡∏≠‡∏≤‡∏à‡πÉ‡∏ä‡πâ", "‡πÉ‡∏ä‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠", "‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏±‡∏Å‡∏©‡∏≤", "‡∏£‡∏±‡∏Å‡∏©‡∏≤‡πÇ‡∏£‡∏Ñ",
                "‡∏ö‡∏£‡∏£‡πÄ‡∏ó‡∏≤‡∏≠‡∏≤‡∏Å‡∏≤‡∏£", "‡∏ä‡πà‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°", "‡∏ä‡πà‡∏ß‡∏¢‡∏•‡∏î‡∏≠‡∏≤‡∏Å‡∏≤‡∏£", "‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô"
            ]
            
            for sentence in sentences:
                sentence = sentence.strip()
                if (len(sentence) > 20 and 
                    any(keyword in sentence for keyword in usage_keywords)):
                    clean_sentence = clean_bullet_text(sentence)
                    if clean_sentence and clean_sentence not in seen:
                        bullets.append(clean_sentence)
                        seen.add(clean_sentence)
        
        return bullets
        
    except Exception as e:
        debug_print(f"Bullet extraction failed: {e}")
        return []

def clean_bullet_text(text):
    """‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° bullet"""
    try:
        # ‡∏•‡∏ö HTML tags
        text = re.sub(r'<[^>]+>', '', text)
        
        # ‡∏•‡∏ö‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
        text = re.sub(r'^[\*‚Ä¢\-\s]+', '', text)
        unwanted = ["‡πÑ‡∏î‡πâ‡πÅ‡∏Å‡πà", "‡∏Ñ‡∏∑‡∏≠", "‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏î‡πâ‡∏ß‡∏¢", "‡πÅ‡∏™‡∏î‡∏á ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°", "‡∏ã‡πà‡∏≠‡∏ô", "‡πÄ‡∏ä‡πà‡∏ô"]
        for word in unwanted:
            text = text.replace(word, "")
        
        # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á
        text = " ".join(text.split()).strip()
        text = text.rstrip('.,;:')
        
        return text if len(text) > 15 else ""
        
    except Exception as e:
        debug_print(f"Clean bullet failed: {e}")
        return text

def clean_single_line_content(content):
    """‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÄ‡∏î‡∏µ‡∏¢‡∏ß"""
    if not content:
        return ""
    
    # ‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
    unwanted = ["‡∏ã‡πà‡∏≠‡∏ô ‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠", "‡πÅ‡∏™‡∏î‡∏á ‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠", "[-]", "[+]", "‡πÅ‡∏™‡∏î‡∏á ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°", "‡∏ã‡πà‡∏≠‡∏ô", "(‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó)"]
    for phrase in unwanted:
        content = content.replace(phrase, "")
    
    # ‡∏ï‡∏±‡∏î‡∏ó‡∏µ‡πà‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
    stop_indicators = ["‡∏¢‡∏≤‡∏ô‡∏µ‡πâ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö", "‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏¢‡∏≤", "‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏£‡πÅ‡∏à‡πâ‡∏á"]
    for indicator in stop_indicators:
        if indicator in content:
            content = content.split(indicator)[0].strip()
            break
    
    # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á
    return " ".join(content.split()).strip()

def format_as_bullets(content_lines):
    """‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÄ‡∏õ‡πá‡∏ô bullet points"""
    if isinstance(content_lines, str):
        content_lines = [content_lines]
    return "\n".join([f"* {line.strip()}" for line in content_lines if line.strip()])

def extract_sections_with_patterns(lines, data):
    """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏¢‡∏≤‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏à‡∏≤‡∏Å div.bs-callout"""
    try:
        section_patterns = {
            "‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏°‡∏±‡∏ç": [r"‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏°‡∏±‡∏ç\s*$", r"GENERIC NAME\s*$"],
            "‡∏ä‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏≤": [r"‡∏ä‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏≤\s*$", r"TRADE NAME\s*$"],
            "‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏¢‡∏≤": [r"‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏¢‡∏≤\s*$", r"DOSAGE FORM\s*$", r"(?:‡∏¢‡∏≤)?‡πÄ‡∏à‡∏•\s*$", r"(?:‡∏ä‡∏ô‡∏¥‡∏î)?‡πÄ‡∏à‡∏•\s*$"],
            "‡∏¢‡∏≤‡∏ô‡∏µ‡πâ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö": [r"‡∏¢‡∏≤‡∏ô‡∏µ‡πâ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö\s*$"],
            "‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏¢‡∏≤": [r"‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏¢‡∏≤\s*$"],
            "‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏£‡πÅ‡∏à‡πâ‡∏á‡πÉ‡∏´‡πâ‡πÅ‡∏û‡∏ó‡∏¢‡πå‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏†‡∏™‡∏±‡∏ä‡∏Å‡∏£‡∏ó‡∏£‡∏≤‡∏ö": [r"‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏£‡πÅ‡∏à‡πâ‡∏á‡πÉ‡∏´‡πâ‡πÅ‡∏û‡∏ó‡∏¢‡πå‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏†‡∏™‡∏±‡∏ä‡∏Å‡∏£‡∏ó‡∏£‡∏≤‡∏ö\s*$"],
            "‡∏ó‡∏≥‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£‡∏´‡∏≤‡∏Å‡∏•‡∏∑‡∏°‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏ó‡∏≤‡∏ô‡∏¢‡∏≤‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏ä‡πâ‡∏¢‡∏≤": [r"‡∏ó‡∏≥‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£‡∏´‡∏≤‡∏Å‡∏•‡∏∑‡∏°‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏ó‡∏≤‡∏ô‡∏¢‡∏≤‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏ä‡πâ‡∏¢‡∏≤\s*$"],
            "‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡πÑ‡∏°‡πà‡∏û‡∏∂‡∏á‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ": [r"‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡πÑ‡∏°‡πà‡∏û‡∏∂‡∏á‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ\s*$"],
            "‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡πÑ‡∏°‡πà‡∏û‡∏∂‡∏á‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏à‡πâ‡∏á‡πÅ‡∏û‡∏ó‡∏¢‡πå‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏†‡∏™‡∏±‡∏ä‡∏Å‡∏£‡∏ó‡∏±‡∏ô‡∏ó‡∏µ": [r"‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡πÑ‡∏°‡πà‡∏û‡∏∂‡∏á‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏à‡πâ‡∏á‡πÅ‡∏û‡∏ó‡∏¢‡πå‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏†‡∏™‡∏±‡∏ä‡∏Å‡∏£‡∏ó‡∏±‡∏ô‡∏ó‡∏µ\s*$"],
            "‡∏Å‡∏≤‡∏£‡πÄ‡∏Å‡πá‡∏ö‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏¢‡∏≤": [r"‡∏Å‡∏≤‡∏£‡πÄ‡∏Å‡πá‡∏ö‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏¢‡∏≤\s*$"]
        }
        
        # ‡∏´‡∏≤‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠
        section_indices = {}
        current_section = None
        section_content = []
        
        for i, line in enumerate(lines):
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡πÉ‡∏´‡∏°‡πà‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            is_new_section = False
            new_section = None
            
            for section_name, patterns in section_patterns.items():
                for pattern in patterns:
                    if re.search(pattern, line, re.IGNORECASE):
                        is_new_section = True
                        new_section = section_name
                        break
                if is_new_section:
                    break
            
            # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏à‡∏≠‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡πÉ‡∏´‡∏°‡πà
            if is_new_section:
                # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏Ç‡∏≠‡∏á‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤
                if current_section and section_content:
                    content = clean_and_format_content(section_content)
                    if content:
                        data[current_section] = content
                
                # ‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÉ‡∏´‡∏°‡πà
                current_section = new_section
                section_content = []
            # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
            elif current_section and line.strip():
                # ‡∏Ç‡πâ‡∏≤‡∏°‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏°‡∏ô‡∏π
                if not any(skip in line for skip in ["‡∏´‡∏ô‡πâ‡∏≤‡∏´‡∏•‡∏±‡∏Å", "‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠‡πÄ‡∏£‡∏≤", "‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ", "‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏¢‡∏≤"]):
                    section_content.append(line)
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏Ç‡∏≠‡∏á‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
        if current_section and section_content:
            content = clean_and_format_content(section_content)
            if content:
                data[current_section] = content
                
    except Exception as e:
        if DEBUG_MODE:
            debug_print(f"Error in extract_sections_with_patterns: {e}")

def extract_administration_bullets_comprehensive(text):
    """‡∏î‡∏∂‡∏á bullet points ‡∏Ç‡∏≠‡∏á‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏¢‡∏≤‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°"""
    try:
        bullets = []
        seen = set()
        
        # 1. ‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å HTML bullet points
        html_bullets = re.findall(r'<li>(.*?)</li>', text, re.DOTALL)
        if html_bullets:
            for bullet in html_bullets:
                clean_bullet = clean_bullet_text(bullet)
                if clean_bullet and clean_bullet not in seen:
                    bullets.append(clean_bullet)
                    seen.add(clean_bullet)
        
        # 2. ‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢ bullet
        if not bullets:
            bullet_markers = ['‚Ä¢', '¬∑', '*', '-', '‚óã', '‚óè', '‚ñ†', '‚ñ™', '‚ó¶']
            for marker in bullet_markers:
                if marker in text:
                    marker_bullets = [b.strip() for b in text.split(marker) if b.strip()]
                    for bullet in marker_bullets:
                        clean_bullet = clean_bullet_text(bullet)
                        if clean_bullet and clean_bullet not in seen:
                            bullets.append(clean_bullet)
                            seen.add(clean_bullet)
                    if bullets:
                        break
        
        # 3. ‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á
        if not bullets:
            sentences = re.split(r'[.!?]', text)
            admin_keywords = [
                "‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏ó‡∏≤‡∏ô", "‡∏â‡∏µ‡∏î", "‡∏ó‡∏≤", "‡∏û‡πà‡∏ô", "‡∏õ‡πâ‡∏≤‡∏¢",
                "‡∏Ç‡∏ô‡∏≤‡∏î‡∏¢‡∏≤", "‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ", "‡πÉ‡∏ä‡πâ‡∏¢‡∏≤", "‡∏Å‡πà‡∏≠‡∏ô‡∏≠‡∏≤‡∏´‡∏≤‡∏£", "‡∏´‡∏•‡∏±‡∏á‡∏≠‡∏≤‡∏´‡∏≤‡∏£"
            ]
            
            for sentence in sentences:
                sentence = sentence.strip()
                if (len(sentence) > 20 and 
                    any(keyword in sentence for keyword in admin_keywords)):
                    clean_sentence = clean_bullet_text(sentence)
                    if clean_sentence and clean_sentence not in seen:
                        bullets.append(clean_sentence)
                        seen.add(clean_sentence)
        
        return bullets
        
    except Exception as e:
        debug_print(f"Administration bullet extraction failed: {e}")
        return []

def is_unwanted_line(line):
    """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£"""
    unwanted_patterns = [
        r"^‡πÅ‡∏™‡∏î‡∏á ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°", r"^‡∏ã‡πà‡∏≠‡∏ô", r"^\[-\]$", r"^\[\+\]$",
        r"^‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠", r"^¬©.*‡∏™‡∏á‡∏ß‡∏ô‡∏•‡∏¥‡∏Ç‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå",
        r"^‡∏ó‡∏£‡∏±‡∏û‡∏¢‡πå‡∏™‡∏¥‡∏ô‡∏ó‡∏≤‡∏á‡∏õ‡∏±‡∏ç‡∏ç‡∏≤", r"^‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ã‡πâ‡∏≥", r"^‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ"
    ]
    
    return any(re.match(pattern, line, re.IGNORECASE) for pattern in unwanted_patterns)

def clean_trade_names_in_files():
    """‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏ä‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏≤‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà"""
    try:
        # ‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î
        files_to_clean = [
            "drug_full_details.json",
            "drug_backup.json",
            "drug_full_details.csv",
            "drug_backup.csv"
        ]
        
        for filename in files_to_clean:
            if not os.path.exists(filename):
                continue
                
            logger.info(f"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÑ‡∏ü‡∏•‡πå {filename}")
            
            # ‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
            if filename.endswith('.json'):
                with open(filename, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    
                # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏ä‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏≤
                for item in data:
                    if "‡∏ä‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏≤" in item:
                        item["‡∏ä‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏≤"] = item["‡∏ä‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏≤"].replace("(‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó)", "").strip()
                
                # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Å‡∏•‡∏±‡∏ö
                with open(filename, 'w', encoding='utf-8') as f:
                    json.dump(data, f, ensure_ascii=False, indent=2)
                    
            elif filename.endswith('.csv'):
                rows = []
                with open(filename, 'r', encoding='utf-8-sig') as f:
                    reader = csv.DictReader(f)
                    fieldnames = reader.fieldnames
                    for row in reader:
                        if "‡∏ä‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏≤" in row:
                            row["‡∏ä‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏≤"] = row["‡∏ä‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏≤"].replace("(‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó)", "").strip()
                        rows.append(row)
                
                # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Å‡∏•‡∏±‡∏ö
                with open(filename, 'w', encoding='utf-8-sig', newline='') as f:
                    writer = csv.DictWriter(f, fieldnames=fieldnames)
                    writer.writeheader()
                    writer.writerows(rows)
            
            logger.info(f"‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÑ‡∏ü‡∏•‡πå {filename} ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô")
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå txt ‡πÉ‡∏´‡∏°‡πà
        if os.path.exists("drug_full_details.json"):
            with open("drug_full_details.json", 'r', encoding='utf-8') as f:
                data = json.load(f)
            save_to_txt(data, "drug_full_details.txt")
            
        if os.path.exists("drug_backup.json"):
            with open("drug_backup.json", 'r', encoding='utf-8') as f:
                data = json.load(f)
            save_to_txt(data, "drug_backup.txt")
            
        logger.info("‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô")
        return True
        
    except Exception as e:
        logger.error(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÑ‡∏ü‡∏•‡πå: {e}")
        return False

def clean_existing_data_content():
    """‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà"""
    try:
        # ‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î
        files_to_clean = [
            "drug_full_details.json",
            "drug_backup.json",
            "drug_full_details.csv",
            "drug_backup.csv"
        ]
        
        # ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î
        sections_to_clean = [
            "‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡πÑ‡∏°‡πà‡∏û‡∏∂‡∏á‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ",
            "‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡πÑ‡∏°‡πà‡∏û‡∏∂‡∏á‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏à‡πâ‡∏á‡πÅ‡∏û‡∏ó‡∏¢‡πå‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏†‡∏™‡∏±‡∏ä‡∏Å‡∏£‡∏ó‡∏±‡∏ô‡∏ó‡∏µ"
        ]
        
        # ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏•‡∏ö (‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏à‡∏≤‡∏Å‡∏¢‡∏≤‡∏ß‡πÑ‡∏õ‡∏™‡∏±‡πâ‡∏ô)
        patterns_to_remove = [
            r"^\*?\s*(?:‡∏°‡∏µ‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡πÑ‡∏°‡πà‡∏û‡∏∂‡∏á‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ|‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡πÑ‡∏°‡πà‡∏û‡∏∂‡∏á‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ)?\s*(?:‡∏°‡∏µ)?(?:‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ|‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ|‡∏î‡∏±‡∏á‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ)\s*[:Ôºö]*",
            r"^\*?\s*(?:‡∏°‡∏µ‡∏≠‡∏≤‡∏Å‡∏≤‡∏£|‡∏≠‡∏≤‡∏Å‡∏≤‡∏£)?\s*(?:‡∏°‡∏µ)?(?:‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ|‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ|‡∏î‡∏±‡∏á‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ)\s*[:Ôºö]*",
            r"^\*?\s*(?:‡∏°‡∏µ)?(?:‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ|‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ|‡∏î‡∏±‡∏á‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ)\s*[:Ôºö]*",
            r"^\*?\s*(?:‡πÑ‡∏î‡πâ‡πÅ‡∏Å‡πà|‡πÄ‡∏ä‡πà‡∏ô|‡∏≠‡∏≤‡∏ó‡∏¥)\s*[:Ôºö]*",
            r"^\*?\s*(?:‡∏û‡∏ö‡πÑ‡∏î‡πâ|‡∏≠‡∏≤‡∏à‡∏û‡∏ö|‡∏°‡∏±‡∏Å‡∏û‡∏ö)\s*[:Ôºö]*",
            r"^[\*\s]*$",  # ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÅ‡∏ï‡πà * ‡πÅ‡∏•‡∏∞‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á
            r"^\*\s*$",    # ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÅ‡∏Ñ‡πà *
            r"^\s*\*\s*$", # ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÅ‡∏Ñ‡πà * ‡πÅ‡∏•‡∏∞‡∏≠‡∏≤‡∏à‡∏°‡∏µ‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á
            r"^‡∏°‡∏µ‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ\s*[:Ôºö]*",  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÉ‡∏´‡∏°‡πà
            r"^‡∏°‡∏µ\s*‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ\s*[:Ôºö]*",  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÉ‡∏´‡∏°‡πà
            r"^‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ\s*[:Ôºö]*",  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÉ‡∏´‡∏°‡πà
            r"^\*\s*‡∏°‡∏µ‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ\s*[:Ôºö]*",  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÉ‡∏´‡∏°‡πà
            r"^\*\s*‡∏°‡∏µ\s*‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ\s*[:Ôºö]*",  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÉ‡∏´‡∏°‡πà
            r"^\*\s*‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ\s*[:Ôºö]*"  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÉ‡∏´‡∏°‡πà
            r"^\*\s‡∏°‡∏µ‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ\s*[:Ôºö]*"  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÉ‡∏´‡∏°‡πà
        ]
        
        def clean_line_content(line):
            """‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î"""
            if not line or line.isspace():
                return ""
                
            # ‡∏•‡∏ö‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏≠‡∏≠‡∏Å
            clean_line = line.strip()
            for pattern in patterns_to_remove:
                clean_line = re.sub(pattern, "", clean_line, flags=re.IGNORECASE).strip()
            
            # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÅ‡∏Ñ‡πà‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏ß‡∏£‡∏£‡∏Ñ‡∏ï‡∏≠‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á ‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≤‡∏°
            if not clean_line or clean_line in [":", "Ôºö", "-", ".", ",", ";"]:
                return ""
            
            # ‡∏•‡∏ö * ‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏ô‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î
            clean_line = re.sub(r'^\*\s*', '', clean_line).strip()
            
            # ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡∏°‡∏µ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏≠‡∏¢‡∏π‡πà ‡πÉ‡∏´‡πâ‡πÄ‡∏û‡∏¥‡πà‡∏° * ‡∏ô‡∏≥‡∏´‡∏ô‡πâ‡∏≤
            return f"* {clean_line}" if clean_line else ""
        
        for filename in files_to_clean:
            if not os.path.exists(filename):
                continue
                
            logger.info(f"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÑ‡∏ü‡∏•‡πå {filename}")
            
            # ‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
            if filename.endswith('.json'):
                with open(filename, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    
                # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤
                for item in data:
                    for section in sections_to_clean:
                        if section in item and isinstance(item[section], str):
                            lines = item[section].split('\n')
                            cleaned_lines = []
                            
                            for line in lines:
                                clean_line = clean_line_content(line)
                                if clean_line:
                                    cleaned_lines.append(clean_line)
                            
                            item[section] = '\n'.join(cleaned_lines) if cleaned_lines else ""
                
                # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Å‡∏•‡∏±‡∏ö
                with open(filename, 'w', encoding='utf-8') as f:
                    json.dump(data, f, ensure_ascii=False, indent=2)
                    
            elif filename.endswith('.csv'):
                rows = []
                with open(filename, 'r', encoding='utf-8-sig') as f:
                    reader = csv.DictReader(f)
                    fieldnames = reader.fieldnames
                    for row in reader:
                        for section in sections_to_clean:
                            if section in row and isinstance(row[section], str):
                                lines = row[section].split('\n')
                                cleaned_lines = []
                                
                                for line in lines:
                                    clean_line = clean_line_content(line)
                                    if clean_line:
                                        cleaned_lines.append(clean_line)
                                
                                row[section] = '\n'.join(cleaned_lines) if cleaned_lines else ""
                        rows.append(row)
                
                # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Å‡∏•‡∏±‡∏ö
                with open(filename, 'w', encoding='utf-8-sig', newline='') as f:
                    writer = csv.DictWriter(f, fieldnames=fieldnames)
                    writer.writeheader()
                    writer.writerows(rows)
            
            logger.info(f"‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÑ‡∏ü‡∏•‡πå {filename} ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô")
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå txt ‡πÉ‡∏´‡∏°‡πà
        if os.path.exists("drug_full_details.json"):
            with open("drug_full_details.json", 'r', encoding='utf-8') as f:
                data = json.load(f)
            save_to_txt(data, "drug_full_details.txt")
            
        if os.path.exists("drug_backup.json"):
            with open("drug_backup.json", 'r', encoding='utf-8') as f:
                data = json.load(f)
            save_to_txt(data, "drug_backup.txt")
            
        logger.info("‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô")
        return True
        
    except Exception as e:
        logger.error(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÑ‡∏ü‡∏•‡πå: {e}")
        return False

# ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÉ‡∏ô‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô main
if __name__ == "__main__":
    if len(sys.argv) > 1:
        command = sys.argv[1]
        
        if command == "clean":
            # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà
            clean_existing_data_content()
            
        elif command == "debug":
            set_debug_mode(True)
            if len(sys.argv) > 2:
                if sys.argv[2] == "url" and len(sys.argv) > 3:
                    test_mode("url", sys.argv[3])
                else:
                    test_mode("quick", sys.argv[2] if len(sys.argv) > 2 else "aa")
            else:
                main()
                
        elif command == "quick":
            keyword = sys.argv[2] if len(sys.argv) > 2 else "aa"
            test_mode("quick", keyword)
            
        elif command == "url" and len(sys.argv) > 2:
            test_mode("url", sys.argv[2])
            
        elif command == "usage":
            # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡πâ‡∏ß‡∏¢‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á
            test_text = """
            ‡∏¢‡∏≤‡∏ô‡∏µ‡πâ‡πÉ‡∏ä‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏±‡∏Å‡∏©‡∏≤‡πÇ‡∏£‡∏Ñ‡∏ï‡∏¥‡∏î‡πÄ‡∏ä‡∏∑‡πâ‡∏≠‡πÅ‡∏ö‡∏Ñ‡∏ó‡∏µ‡πÄ‡∏£‡∏µ‡∏¢ ‡πÄ‡∏ä‡πà‡∏ô ‡πÇ‡∏£‡∏Ñ‡∏õ‡∏≠‡∏î‡∏≠‡∏±‡∏Å‡πÄ‡∏™‡∏ö ‡πÇ‡∏£‡∏Ñ‡∏´‡∏•‡∏≠‡∏î‡∏•‡∏°‡∏≠‡∏±‡∏Å‡πÄ‡∏™‡∏ö ‡πÇ‡∏£‡∏Ñ‡∏ï‡∏¥‡∏î‡πÄ‡∏ä‡∏∑‡πâ‡∏≠‡∏ó‡∏µ‡πà‡∏´‡∏π ‡∏à‡∏°‡∏π‡∏Å ‡∏•‡∏≥‡∏Ñ‡∏≠ ‡∏ú‡∏¥‡∏ß‡∏´‡∏ô‡∏±‡∏á ‡∏£‡∏∞‡∏ö‡∏ö‡∏ó‡∏≤‡∏á‡πÄ‡∏î‡∏¥‡∏ô‡∏õ‡∏±‡∏™‡∏™‡∏≤‡∏ß‡∏∞ ‡πÇ‡∏£‡∏Ñ‡∏´‡∏ô‡∏≠‡∏á‡πÉ‡∏ô‡πÄ‡∏ó‡∏µ‡∏¢‡∏°‡∏Ñ‡∏•‡∏≤‡∏°‡∏±‡∏¢‡πÄ‡∏î‡∏µ‡∏¢ ‡∏ó‡∏£‡∏≤‡πÇ‡∏Ñ‡∏°‡∏≤‡∏ó‡∏¥‡∏™ (chlamydia trachomatis) (‡πÄ‡∏õ‡πá‡∏ô‡∏¢‡∏≤‡∏ó‡∏≤‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÉ‡∏ô‡∏´‡∏ç‡∏¥‡∏á‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡∏£‡∏£‡∏†‡πå‡∏ó‡∏µ‡πà‡πÅ‡∏û‡∏ó‡∏¢‡πå‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏ñ‡∏∂‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô)
            ‡∏≠‡∏≤‡∏à‡πÉ‡∏ä‡πâ‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ö‡∏¢‡∏≤‡∏≠‡∏∑‡πà‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏±‡∏Å‡∏©‡∏≤‡πÇ‡∏£‡∏Ñ‡∏Å‡∏£‡∏∞‡πÄ‡∏û‡∏≤‡∏∞‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡πÄ‡∏ä‡∏∑‡πâ‡∏≠‡πÄ‡∏Æ‡∏•‡∏¥‡πÇ‡∏Ñ‡πÅ‡∏ö‡∏Ñ‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÑ‡∏û‡πÇ‡∏•‡πÑ‡∏£ (H.pylori)
            ‡∏¢‡∏≤‡∏ô‡∏µ‡πâ‡∏≠‡∏≤‡∏à‡πÉ‡∏ä‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏±‡∏Å‡∏©‡∏≤‡πÇ‡∏£‡∏Ñ‡∏´‡∏£‡∏∑‡∏≠‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏≠‡∏∑‡πà‡∏ô‡πÜ‡πÑ‡∏î‡πâ ‡∏´‡∏≤‡∏Å‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏™‡∏á‡∏™‡∏±‡∏¢‡∏Ñ‡∏ß‡∏£‡∏õ‡∏£‡∏∂‡∏Å‡∏©‡∏≤‡πÅ‡∏û‡∏ó‡∏¢‡πå‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏†‡∏™‡∏±‡∏ä‡∏Å‡∏£
            """
            
            print("üß™ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• '‡∏¢‡∏≤‡∏ô‡∏µ‡πâ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö'")
            set_debug_mode(True)
            
            bullets = extract_usage_bullets_comprehensive(test_text.strip())
            result = "\n".join([f"* {bullet}" for bullet in bullets])
            
            print("\nüìã ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå:")
            print(result)
            print(f"\nüìä ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô bullet points: {len(bullets)}")
            
        elif command == "help":
            print("üìñ ‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô:")
            print("  python script.py                    # ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏õ‡∏Å‡∏ï‡∏¥")
            print("  python script.py quick [keyword]    # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÄ‡∏£‡πá‡∏ß (default: aa)")
            print("  python script.py url [URL]          # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö URL ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á")
            print("  python script.py debug [keyword]    # ‡πÇ‡∏´‡∏°‡∏î debug")
            print("  python script.py debug url [URL]    # debug URL ‡πÄ‡∏â‡∏û‡∏≤‡∏∞")
            print("  python script.py usage              # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÅ‡∏¢‡∏Å '‡∏¢‡∏≤‡∏ô‡∏µ‡πâ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö'")
            print("  python script.py help               # ‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏´‡∏•‡∏∑‡∏≠")
        else:
            main()
    else:
        main()